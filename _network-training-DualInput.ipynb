{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b7c6ff",
   "metadata": {},
   "source": [
    "### Multi-input Network - Pytorch\n",
    "\n",
    "Prediction action sequence that connects two observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4b68e",
   "metadata": {},
   "source": [
    "### Important\n",
    "\n",
    "- unify train function for the different training modalities !\n",
    "\n",
    "###### waypoint executer\n",
    "- write program that finds path between waypoints (using balck and white image as map the agent can walk on (?))\n",
    "\n",
    "###### data\n",
    "- check image normalization (with transform!): rescale /255\n",
    "- black and white ?\n",
    "\n",
    "###### training\n",
    "- Build network that predicts multiple actions (both as vector and with labels) \n",
    "- check whether difference image gives sensible predictions\n",
    "- are weights shared correctly within the two input networks?\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "### Optional\n",
    "\n",
    "###### structure\n",
    "- look into general structure of the NeurIPS project, Martius Project (https://github.com/martius-lab/PPGS) or Dreamer\n",
    "\n",
    "###### saving\n",
    "- create hash and .csv or .md file that collects all the different saved networks (and their associated parameter settings)\n",
    "\n",
    "###### training\n",
    "- write dict to pass to train function with parameters (+ possibility to have addititonal outputs)\n",
    "- Feedforward network likely could be improved by making use of dropout\n",
    "- autosave models (in runs folder!)\n",
    "- fix the naming (of how models are saved, configs are saved, runs are called)\n",
    "\n",
    "###### tensorboard\n",
    "- write tensorboard integration for gradients\n",
    "- collect more image visuals / intermediate predictions (compute confusion matrix?)\n",
    "\n",
    "###### colab\n",
    "- train models using colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imports external and own libraries\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# own\n",
    "import collector\n",
    "import action\n",
    "import world\n",
    "import plot\n",
    "import preprocess\n",
    "import nets\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8277f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load data with pickle (deserialize)\n",
    "'''\n",
    "with open('datasets/oracle_data.pickle', 'rb') as handle:\n",
    "    oracle_data = pickle.load(handle)\n",
    "\n",
    "with open('datasets/oracle_reversed_data.pickle', 'rb') as handle:\n",
    "    oracle_reversed_data = pickle.load(handle)\n",
    "\n",
    "with open('datasets/oracle_random_data.pickle', 'rb') as handle:\n",
    "    oracle_random_data = pickle.load(handle)\n",
    "\n",
    "with open('datasets/oracle_reversed_random_data.pickle', 'rb') as handle:\n",
    "    oracle_reversed_random_data = pickle.load(handle)\n",
    "    \n",
    "with open('datasets/random_data.pickle', 'rb') as handle:\n",
    "    random_data = pickle.load(handle)\n",
    "    \n",
    "with open('datasets/tmaze_random_reverse_data.pickle', 'rb') as handle:\n",
    "    tmaze_random_reverse_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a2921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use Dataloader to make the data ready for the training loop\n",
    "'''\n",
    "\n",
    "dataset = preprocess.split_n_steps_between(tmaze_random_reverse_data, n=2)\n",
    "\n",
    "single_class_encoding_dic = {}\n",
    "counter = 0\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        single_class_encoding_dic[str([i, j])] = counter\n",
    "        counter += 1\n",
    "\n",
    "actions_recoded = []\n",
    "for actions in dataset['actions']:\n",
    "    actions_recoded.append([single_class_encoding_dic[str(actions)]])\n",
    "    \n",
    "dataset['actions'] = actions_recoded\n",
    "\n",
    "train_data, test_data = preprocess.split_data(dataset, 0.8)\n",
    "\n",
    "# preprocess trainingset \n",
    "oracle_train_data = preprocess.ObtainDualDataset(train_data, 'observationsA','observationsB', 'actions')\n",
    "oracle_test_data = preprocess.ObtainDualDataset(test_data, 'observationsA','observationsB', 'actions')\n",
    "\n",
    "# build dataloader (tensor format)\n",
    "batch_size = 64\n",
    "dataset_loader_train_data = DataLoader(oracle_train_data, batch_size=batch_size, shuffle=True)\n",
    "dataset_loader_test_data = DataLoader(oracle_test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca411475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "imagesA, imagesB, labels = next(iter(oracle_train_data))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(imagesA[0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imagesB[0])\n",
    "plt.show()\n",
    "\n",
    "print(labels)\n",
    "\n",
    "print(len(imagesA))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82934949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             456\n",
      "         MaxPool2d-2            [-1, 6, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 10, 10]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 5, 5]               0\n",
      "            Conv2d-5            [-1, 6, 28, 28]             456\n",
      "         MaxPool2d-6            [-1, 6, 14, 14]               0\n",
      "            Conv2d-7           [-1, 16, 10, 10]           2,416\n",
      "         MaxPool2d-8             [-1, 16, 5, 5]               0\n",
      "            Linear-9                  [-1, 120]          96,120\n",
      "           Linear-10                   [-1, 84]          10,164\n",
      "           Linear-11                   [-1, 16]           1,360\n",
      "================================================================\n",
      "Total params: 113,388\n",
      "Trainable params: 113,388\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 36.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 0.43\n",
      "Estimated Total Size (MB): 36.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Initialize dual input model\n",
    "'''\n",
    "\n",
    "DualInput_model = nets.DualInput()\n",
    "summary(DualInput_model, [(3,32,32), (3,32,32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make use of TensorBoard for visualizing logged results\n",
    "'''\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b39774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torch\n",
    "import plot\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def DualInput(train_data, val_data, net, criterion, optimizer, steps):\n",
    "    '''\n",
    "    Main training loop\n",
    "    Input: dataset_loader, network, training_loss, optimizer, step size\n",
    "    Output: trained network\n",
    "    '''\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    imagesA, imagesB, labels = next(iter(train_data))\n",
    "    grid = torchvision.utils.make_grid(imagesA)\n",
    "    writer.add_image('images', grid, 0)\n",
    "    writer.add_graph(net, [imagesA, imagesB])\n",
    "\n",
    "    with tqdm(total=steps, unit =\" Episode\", desc =\"Progress\") as pbar:\n",
    "        for epoch in range(steps):  # loop over the dataset multiple times\n",
    "\n",
    "            train_running_loss = 0.0\n",
    "\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for i, data in enumerate(train_data, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputsA, inputB, labels = data\n",
    "                labels = labels[0]\n",
    "                labels = labels.to(torch.long)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputsA, inputB)\n",
    "\n",
    "                # loss\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # compute acc\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                # track loss statistics\n",
    "                train_running_loss += loss.item()\n",
    "\n",
    "                test_running_loss = 0.0\n",
    "\n",
    "                test_correct = 0\n",
    "                test_total = 0\n",
    "\n",
    "                # same for validation set\n",
    "                with torch.no_grad():\n",
    "                    for data in val_data:\n",
    "                        inputsA, inputsB, labels = data\n",
    "                        labels = labels[0]\n",
    "                        labels = labels.to(torch.long)\n",
    "                        \n",
    "                        outputs = net(inputsA, inputsB)\n",
    "                        loss = criterion(outputs.squeeze(), labels)\n",
    "                        test_running_loss += loss.item()\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        test_total += labels.size(0)\n",
    "                        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # tensorboard logs\n",
    "            writer.add_scalar('Loss/train', (train_running_loss/len(train_data)), epoch)\n",
    "            writer.add_scalar('Loss/test', (test_running_loss/len(val_data)), epoch)\n",
    "            writer.add_scalar('Accuracy/train', (100 * train_correct / train_total), epoch)\n",
    "            writer.add_scalar('Accuracy/test', (100 * test_correct / test_total), epoch)\n",
    "\n",
    "            # plotting logs\n",
    "            train_loss.append(train_running_loss/len(train_data))\n",
    "            test_loss.append(test_running_loss/len(val_data))\n",
    "            train_acc.append(100 * train_correct / train_total)\n",
    "            test_acc.append(100 * test_correct / test_total)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch: {epoch + 1}, Train Loss: {(train_running_loss/len(train_data)):.4}, Train Acc: {(100 * train_correct / train_total):.4} %,  Test Loss: {(test_running_loss/len(val_data)):.4}, Test Acc: {(100 * test_correct / test_total):.4} %,')\n",
    "\n",
    "    writer.close()\n",
    "    print('Finished Training')\n",
    "    return net, train_loss, test_loss, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed13d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 800])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([16, 84])\n",
      "fc3.bias \t torch.Size([16])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df33135c42045beb1949bf144459904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/500 [00:00<?, ? Episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.776, Train Acc: 5.441 %,  Test Loss: 2.778, Test Acc: 6.516 %,\n",
      "Epoch: 11, Train Loss: 2.769, Train Acc: 7.004 %,  Test Loss: 2.78, Test Acc: 5.514 %,\n",
      "Epoch: 21, Train Loss: 2.769, Train Acc: 7.129 %,  Test Loss: 2.783, Test Acc: 5.514 %,\n",
      "Epoch: 31, Train Loss: 2.769, Train Acc: 7.129 %,  Test Loss: 2.781, Test Acc: 5.514 %,\n",
      "Epoch: 41, Train Loss: 2.769, Train Acc: 7.004 %,  Test Loss: 2.78, Test Acc: 6.516 %,\n",
      "Epoch: 51, Train Loss: 2.769, Train Acc: 7.442 %,  Test Loss: 2.78, Test Acc: 6.767 %,\n",
      "Epoch: 61, Train Loss: 2.769, Train Acc: 6.942 %,  Test Loss: 2.784, Test Acc: 6.516 %,\n",
      "Epoch: 71, Train Loss: 2.769, Train Acc: 6.817 %,  Test Loss: 2.778, Test Acc: 6.266 %,\n",
      "Epoch: 81, Train Loss: 2.769, Train Acc: 6.692 %,  Test Loss: 2.785, Test Acc: 6.767 %,\n",
      "Epoch: 91, Train Loss: 2.768, Train Acc: 7.129 %,  Test Loss: 2.785, Test Acc: 6.015 %,\n",
      "Epoch: 101, Train Loss: 2.768, Train Acc: 7.442 %,  Test Loss: 2.782, Test Acc: 6.767 %,\n",
      "Epoch: 111, Train Loss: 2.768, Train Acc: 6.942 %,  Test Loss: 2.787, Test Acc: 6.266 %,\n",
      "Epoch: 121, Train Loss: 2.766, Train Acc: 7.255 %,  Test Loss: 2.781, Test Acc: 5.764 %,\n",
      "Epoch: 131, Train Loss: 2.765, Train Acc: 7.755 %,  Test Loss: 2.783, Test Acc: 5.514 %,\n",
      "Epoch: 141, Train Loss: 2.764, Train Acc: 7.755 %,  Test Loss: 2.784, Test Acc: 7.018 %,\n",
      "Epoch: 151, Train Loss: 2.76, Train Acc: 9.131 %,  Test Loss: 2.786, Test Acc: 7.519 %,\n",
      "Epoch: 161, Train Loss: 2.754, Train Acc: 8.818 %,  Test Loss: 2.782, Test Acc: 6.266 %,\n",
      "Epoch: 171, Train Loss: 2.703, Train Acc: 10.69 %,  Test Loss: 2.766, Test Acc: 7.018 %,\n",
      "Epoch: 181, Train Loss: 2.607, Train Acc: 13.07 %,  Test Loss: 2.695, Test Acc: 9.774 %,\n",
      "Epoch: 191, Train Loss: 2.558, Train Acc: 12.45 %,  Test Loss: 2.681, Test Acc: 9.023 %,\n",
      "Epoch: 201, Train Loss: 2.543, Train Acc: 14.13 %,  Test Loss: 2.654, Test Acc: 8.271 %,\n",
      "Epoch: 211, Train Loss: 2.508, Train Acc: 14.88 %,  Test Loss: 2.678, Test Acc: 9.023 %,\n",
      "Epoch: 221, Train Loss: 2.491, Train Acc: 16.01 %,  Test Loss: 2.658, Test Acc: 9.023 %,\n",
      "Epoch: 231, Train Loss: 2.476, Train Acc: 15.76 %,  Test Loss: 2.671, Test Acc: 8.02 %,\n",
      "Epoch: 241, Train Loss: 2.449, Train Acc: 17.07 %,  Test Loss: 2.655, Test Acc: 9.524 %,\n",
      "Epoch: 251, Train Loss: 2.4, Train Acc: 18.32 %,  Test Loss: 2.774, Test Acc: 7.769 %,\n",
      "Epoch: 261, Train Loss: 2.353, Train Acc: 20.33 %,  Test Loss: 2.735, Test Acc: 9.023 %,\n",
      "Epoch: 271, Train Loss: 2.328, Train Acc: 19.95 %,  Test Loss: 2.707, Test Acc: 10.03 %,\n",
      "Epoch: 281, Train Loss: 2.282, Train Acc: 22.14 %,  Test Loss: 2.785, Test Acc: 10.03 %,\n",
      "Epoch: 291, Train Loss: 2.228, Train Acc: 23.14 %,  Test Loss: 2.977, Test Acc: 8.02 %,\n",
      "Epoch: 301, Train Loss: 2.155, Train Acc: 25.52 %,  Test Loss: 3.007, Test Acc: 7.769 %,\n",
      "Epoch: 311, Train Loss: 2.104, Train Acc: 25.64 %,  Test Loss: 3.07, Test Acc: 7.769 %,\n",
      "Epoch: 321, Train Loss: 2.068, Train Acc: 27.14 %,  Test Loss: 3.117, Test Acc: 9.524 %,\n",
      "Epoch: 331, Train Loss: 1.929, Train Acc: 31.52 %,  Test Loss: 3.229, Test Acc: 10.28 %,\n",
      "Epoch: 341, Train Loss: 1.853, Train Acc: 34.71 %,  Test Loss: 3.564, Test Acc: 8.772 %,\n",
      "Epoch: 351, Train Loss: 1.756, Train Acc: 38.27 %,  Test Loss: 3.895, Test Acc: 9.023 %,\n",
      "Epoch: 361, Train Loss: 1.658, Train Acc: 41.78 %,  Test Loss: 4.108, Test Acc: 10.03 %,\n",
      "Epoch: 371, Train Loss: 1.587, Train Acc: 44.47 %,  Test Loss: 4.427, Test Acc: 9.774 %,\n",
      "Epoch: 381, Train Loss: 1.456, Train Acc: 49.03 %,  Test Loss: 4.493, Test Acc: 8.271 %,\n",
      "Epoch: 391, Train Loss: 1.257, Train Acc: 56.04 %,  Test Loss: 4.814, Test Acc: 8.02 %,\n",
      "Epoch: 401, Train Loss: 1.224, Train Acc: 55.97 %,  Test Loss: 5.146, Test Acc: 11.03 %,\n",
      "Epoch: 411, Train Loss: 1.03, Train Acc: 63.04 %,  Test Loss: 6.005, Test Acc: 10.28 %,\n",
      "Epoch: 421, Train Loss: 0.9716, Train Acc: 66.6 %,  Test Loss: 6.007, Test Acc: 9.774 %,\n",
      "Epoch: 431, Train Loss: 0.8411, Train Acc: 68.98 %,  Test Loss: 7.453, Test Acc: 10.03 %,\n",
      "Epoch: 441, Train Loss: 0.7404, Train Acc: 74.92 %,  Test Loss: 7.14, Test Acc: 9.023 %,\n",
      "Epoch: 451, Train Loss: 0.6197, Train Acc: 77.74 %,  Test Loss: 7.779, Test Acc: 10.03 %,\n",
      "Epoch: 461, Train Loss: 0.6187, Train Acc: 78.55 %,  Test Loss: 8.212, Test Acc: 9.273 %,\n",
      "Epoch: 471, Train Loss: 0.5521, Train Acc: 79.86 %,  Test Loss: 8.528, Test Acc: 7.769 %,\n",
      "Epoch: 481, Train Loss: 0.5234, Train Acc: 82.61 %,  Test Loss: 8.359, Test Acc: 9.774 %,\n",
      "Epoch: 491, Train Loss: 0.3925, Train Acc: 86.49 %,  Test Loss: 9.524, Test Acc: 8.772 %,\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the network\n",
    "- obtained with SGD, lr=0.01, mometum=0.9, 100 episodes: train acc. 50%, test acc. 48%\n",
    "- obtained with SGD, lr=0.01, mometum=0.9, 200 episodes: train acc. 55%, test acc. 37%\n",
    "- obtained with SGD, lr=0.01, mometum=0.9, 300 episodes: train acc. 60%, test acc. 29%\n",
    "'''\n",
    "# DualInput_model = nets.DualInput()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(DualInput_model.parameters(), lr=0.01, momentum=0.9) # RMSProp, Adam, SDG (momentum=0.9)\n",
    "#optimizer = optim.RMSprop(DualInput_model.parameters(), lr=0.001)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in DualInput_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", DualInput_model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"\\nOptimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "episodes = 500\n",
    "DualInput_model, train_loss, test_loss, train_acc, test_acc  = DualInput(dataset_loader_train_data, dataset_loader_test_data, DualInput_model, criterion, optimizer, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plot loss and accuracy curves for training and test set\n",
    "'''\n",
    "\n",
    "plot.plot_losses(train_loss, test_loss)\n",
    "plot.plot_acc(train_acc, test_acc, smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e787ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Show example classifications and plot confusion matrix\n",
    "'''\n",
    "\n",
    "plot.show_example_classificataions(dataset_loader_train_data, DualInput_model, amount=8)\n",
    "plot.plot_confusion_matrix(dataset_loader_train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "classes_expl = {0: 'turn left', 1: 'turn right', 2: 'walk forwards', 3: 'walk backwards'}\n",
    "\n",
    "index = 4\n",
    "\n",
    "imageA = dataset['observationsA'][index]\n",
    "imageB = dataset['observationsB'][index]\n",
    "lstval = [classes_expl[key] for key in dataset['actions'][index]]\n",
    "\n",
    "plt.imshow(imageA)\n",
    "plt.show()\n",
    "\n",
    "print(lstval)\n",
    "\n",
    "plt.imshow(imageB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate into TensorBoard \n",
    "\n",
    "#for name, param in net.named_parameters():\n",
    "#    print(name, param.grad.abs().sum())\n",
    "#print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem: the label needs to be a tensor as well!\n",
    "# important when one wants to make multi step predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many classes do we need?\n",
    "print('single step', 4)\n",
    "print('two steps', 4**2)\n",
    "print('three step', 4**3)\n",
    "print('four step', 4**4)\n",
    "print('five step', 4**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build the dictionary myself!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess images (Normalize inputs)\n",
    "'''\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(oracle_train_data, batch_size=4096, shuffle=False, num_workers=4)\n",
    "\n",
    "pop_mean = []\n",
    "pop_std0 = []\n",
    "pop_std1 = []\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    # shape (batch_size, 3, height, width)\n",
    "    numpy_image = data['image'].numpy()\n",
    "    \n",
    "    # shape (3,)\n",
    "    batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    batch_std0 = np.std(numpy_image, axis=(0,2,3))\n",
    "    batch_std1 = np.std(numpy_image, axis=(0,2,3), ddof=1)\n",
    "    \n",
    "    pop_mean.append(batch_mean)\n",
    "    pop_std0.append(batch_std0)\n",
    "    pop_std1.append(batch_std1)\n",
    "\n",
    "# shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)\n",
    "pop_mean = np.array(pop_mean).mean(axis=0)\n",
    "pop_std0 = np.array(pop_std0).mean(axis=0)\n",
    "pop_std1 = np.array(pop_std1).mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
