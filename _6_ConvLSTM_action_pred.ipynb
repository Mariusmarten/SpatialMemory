{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4231cfd",
   "metadata": {},
   "source": [
    "# Convolutional LSTM for action prediction\n",
    "\n",
    "- keep track of best validation value + Save checkpoints with information (autosave)\n",
    "- clean naming (of how models are saved, configs are saved, runs are called)\n",
    "- confusion matrix of result / comparison with baseline LSTM (that only takes in actions)\n",
    "- wandb/ tensorboard integration (also for gradient information?)\n",
    "- hydra integration for hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1e391",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da778ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hydra\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "from torchinfo import summary\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# own\n",
    "import common.action as action\n",
    "import common.world as world\n",
    "import common.plot as plot\n",
    "import common.preprocess as preprocess\n",
    "import common.nets as nets\n",
    "import common.train as train\n",
    "import common.tools as tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3400d",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a9b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input size is 960 or 1\n",
    "train_imgs = False\n",
    "\n",
    "# preprocessing\n",
    "seq_length = 20\n",
    "training_set_size = 0.67\n",
    "\n",
    "# lstm configuration\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "num_classes = 4\n",
    "if train_imgs:\n",
    "    input_size = 960\n",
    "else:\n",
    "    input_size = 1\n",
    "# training\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "optimizer_type = 'Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d335da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      "  train_imgs: false\n",
      "  seq_length: 20\n",
      "  batch_size: 128\n",
      "  training_set_size: 0.67\n",
      "  hidden_size: 20\n",
      "  num_layers: 1\n",
      "  num_classes: 4\n",
      "  input_size: 1\n",
      "  num_epochs: 2000\n",
      "  learning_rate: 0.01\n",
      "  optimizer_type: Adam\n",
      "  save_model: false\n",
      "  save_plots: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hydra integration for hyperparameters\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "hydra.initialize(version_base=None, config_path='conf') # Assume the configuration file is in the current folder\n",
    "cfg = hydra.compose(config_name='config')\n",
    "# Can be used in the following way: cfg.params.learning_rate \n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309342f",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c755f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/oracle_data.pickle', 'rb') as handle:\n",
    "    oracle_data = pickle.load(handle)\n",
    "\n",
    "with open('datasets/oracle_reversed_data.pickle', 'rb') as handle:\n",
    "    oracle_reversed_data = pickle.load(handle)\n",
    "\n",
    "with open('datasets/oracle_random_data.pickle', 'rb') as handle:\n",
    "    oracle_random_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b6af1",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5db2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess with sequence length\n",
    "x_acts, x_imgs, y_acts = preprocess.sliding_windows(oracle_data, seq_length)\n",
    "# data, train, test split\n",
    "data, train, test = preprocess.split(x_acts, x_imgs, y_acts, training_set_size)\n",
    "dataX_acts, dataX_imgs, dataY_acts = data\n",
    "trainX_acts, trainX_imgs, trainY_acts = train\n",
    "testX_acts, testX_imgs, testY_act = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92aabe8",
   "metadata": {},
   "source": [
    "### Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae5c0ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN input shape: torch.Size([786, 20, 3, 32, 32])\n",
      "CNN output shape: torch.Size([786, 20, 1])\n",
      "LSTM input shape: torch.Size([786, 20, 1])\n",
      "LSTM output shape: torch.Size([786, 4])\n",
      "Label shape: torch.Size([786, 4])\n",
      "SUMMARY CNN \n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "CNN                                      [786, 20, 960]            --\n",
      "├─Conv2d: 1-1                            [15720, 20, 28, 28]       1,520\n",
      "├─Conv2d: 1-2                            [15720, 40, 24, 24]       20,040\n",
      "├─MaxPool2d: 1-3                         [15720, 40, 12, 12]       --\n",
      "├─Conv2d: 1-4                            [15720, 60, 8, 8]         60,060\n",
      "├─MaxPool2d: 1-5                         [15720, 60, 4, 4]         --\n",
      "==========================================================================================\n",
      "Total params: 81,620\n",
      "Trainable params: 81,620\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 260.61\n",
      "==========================================================================================\n",
      "Input size (MB): 193.17\n",
      "Forward/backward pass size (MB): 5352.35\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 5545.84\n",
      "========================================================================================== \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rand() received an invalid combination of arguments - got (tuple, tuple), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, trainY_acts\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUMMARY CNN \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, summary(cnn, trainX_imgs\u001b[38;5;241m.\u001b[39msize()), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUMMARY LSTM \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torchinfo/torchinfo.py:215\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m validate_user_params(\n\u001b[1;32m    212\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    213\u001b[0m )\n\u001b[0;32m--> 215\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[1;32m    219\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    220\u001b[0m )\n\u001b[1;32m    221\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torchinfo/torchinfo.py:250\u001b[0m, in \u001b[0;36mprocess_input\u001b[0;34m(input_data, input_size, batch_dim, device, dtypes)\u001b[0m\n\u001b[1;32m    248\u001b[0m         dtypes \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfloat] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_size)\n\u001b[1;32m    249\u001b[0m     correct_input_size \u001b[38;5;241m=\u001b[39m get_correct_input_sizes(input_size)\n\u001b[0;32m--> 250\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_input_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, correct_input_size\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torchinfo/torchinfo.py:472\u001b[0m, in \u001b[0;36mget_input_tensor\u001b[0;34m(input_size, batch_dim, dtypes, device)\u001b[0m\n\u001b[1;32m    470\u001b[0m x \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, dtype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_size, dtypes):\n\u001b[0;32m--> 472\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39mbatch_dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: rand() received an invalid combination of arguments - got (tuple, tuple), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "cnn = nets.CNN(seq_length)\n",
    "lstm = nets.LSTM(num_classes, input_size, hidden_size, num_layers, seq_length)\n",
    "\n",
    "if train_imgs:\n",
    "    features = cnn(trainX_imgs)\n",
    "else:\n",
    "    features = trainX_acts\n",
    "\n",
    "outputs = lstm(features)\n",
    "\n",
    "print('CNN input shape:', trainX_imgs.size())\n",
    "print('CNN output shape:', features.size())\n",
    "\n",
    "print('LSTM input shape:', features.size())\n",
    "print('LSTM output shape:', outputs.size())\n",
    "\n",
    "print('Label shape:', trainY_acts.size())\n",
    "\n",
    "print('SUMMARY CNN \\n', summary(cnn, trainX_imgs.size()), '\\n')\n",
    "print('SUMMARY LSTM \\n', summary(lstm, (outputs.size(), ((2, 10), (2, 10,)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174417c9",
   "metadata": {},
   "source": [
    "### Tensorboard integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d402f0",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7749c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb and tensorboard integration\n",
    "# write model summary/ parameter settings into runs folder\n",
    "\n",
    "# track best validation value\n",
    "# save model during training\n",
    "\n",
    "# full batch training likely is too slow (?)\n",
    "\"\"\"\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in DualInput_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", DualInput_model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"\\nOptimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f84d5212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbc571a25a44bcd93dc25cb483a7956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/10 [00:00<?, ? Episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Train Loss: 0.487\n",
      "Epoch: 1 - Train Loss: 0.490\n",
      "Epoch: 2 - Train Loss: 0.468\n",
      "Epoch: 3 - Train Loss: 0.467\n",
      "Epoch: 4 - Train Loss: 0.458\n",
      "Epoch: 5 - Train Loss: 0.444\n",
      "Epoch: 6 - Train Loss: 0.437\n",
      "Epoch: 7 - Train Loss: 0.431\n",
      "Epoch: 8 - Train Loss: 0.421\n",
      "Epoch: 9 - Train Loss: 0.411\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "if train_imgs:\n",
    "    params = list(cnn.parameters()) + list(lstm.parameters())\n",
    "else:\n",
    "    params = lstm.parameters()\n",
    "\n",
    "# MSELoss - regression, CrossEntropyLoss for labels or BCEWithLogitsLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if optimizer_type == 'Adam':\n",
    "    optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# save current conf infos\n",
    "#local_time = str(datetime.datetime.now().isoformat())\n",
    "#name = local_time + '_configs'\n",
    "#OmegaConf.save(cfg, \"runs/\"+name)\n",
    "\n",
    "with tqdm(total=num_epochs, unit =\" Episode\", desc =\"Progress\") as pbar:\n",
    "    \n",
    "    # tracking results\n",
    "    train_loss_lst, test_loss_lst = [], []\n",
    "    train_acc_lst, test_acc_lst = [], []\n",
    "    test_best_acc = 100\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_loss, test_loss = 0, 0\n",
    "        \n",
    "        # choose training style\n",
    "        if train_imgs:\n",
    "            features = cnn(trainX_imgs)\n",
    "        else:\n",
    "            features = trainX_acts\n",
    "            \n",
    "        # model\n",
    "        outputs = lstm(features)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # loss + optimize\n",
    "        loss = criterion(outputs, trainY_acts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "                \n",
    "        # display\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch: %d - Train Loss: %1.3f\" % (epoch, loss.item()))\n",
    "        \n",
    "        # tensorboard logs\n",
    "        \n",
    "        # plotting logs\n",
    "            \n",
    "        pbar.update(1)\n",
    "    \n",
    "    writer.close()\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034f4a3",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbe498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_one_hot(data_predict):\n",
    "    new_data_predict = []\n",
    "    for i in data_predict:\n",
    "        if str(i) != '[0. 0. 0. 0.]':\n",
    "            new_data_predict.append(np.where(i==1)[0][0])\n",
    "        else:\n",
    "            new_data_predict.append(None)\n",
    "    return new_data_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "lstm.eval()\n",
    "\n",
    "train_imgs = True\n",
    "if train_imgs:\n",
    "    features = cnn(trainX_imgs)\n",
    "    features = features #+ 0.01*trainX_acts\n",
    "    train_predict = lstm(features)\n",
    "else:\n",
    "    train_predict = lstm(trainX_acts)\n",
    "\n",
    "t = Variable(torch.Tensor([0]))  # threshold\n",
    "train_predict = (train_predict > t).float() * 1\n",
    "\n",
    "data_predict = train_predict.data.numpy()\n",
    "dataY_plot = dataY_acts.data.numpy()\n",
    " \n",
    "new_data_predict = recode_one_hot(data_predict)\n",
    "new_dataY_plot = [np.where(r==1)[0][0] for r in dataY_plot]\n",
    "\n",
    "with plt.style.context('ggplot'):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.axvline(int(len(y_acts) * training_set_size), c='r', linestyle='--')\n",
    "\n",
    "    plt.plot(new_dataY_plot, 'o', color='slategray', markersize=10, label=\"Ground truth\")\n",
    "    plt.plot(new_data_predict, 'o', markersize=5, label=\"Predicted\")\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title('Action sequence prediction')\n",
    "    plt.xlabel('Sequence')\n",
    "    plt.ylabel('Action')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3750533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff5d211",
   "metadata": {},
   "source": [
    "### Plot loss and accuracy curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "pytorch_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
