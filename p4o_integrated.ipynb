{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRsMbJ9qkFop",
    "outputId": "8c5fee91-4965-4b36-bb5e-2b0ece46b773",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install required modules\n",
    "# !pip install mxnet-cu101 opencv-python numpy tensorboard mxboard matplotlib pandas_bokeh gym[atari]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GmWaxJlDkFow",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "from mxnet import nd, gluon, init, autograd\n",
    "from mxnet.gluon.rnn.rnn_cell import _format_sequence, _get_begin_state, _mask_sequence_variable_length\n",
    "\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet as mx\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_bokeh\n",
    "import gc\n",
    "import os\n",
    "import multiprocessing\n",
    "import multiprocessing.connection\n",
    "from mxboard import SummaryWriter\n",
    "import datetime\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "output_name      = 'p4o_integrated'\n",
    "game             = \"SeaquestDeterministic-v4\"\n",
    "stacked_frames   = 4            # Number of stacked frames to use\n",
    "context          = mx.gpu()     # GPU or CPU based training\n",
    "opt_lr           = 2.5e-4       # Adam optimizer learning rate\n",
    "opt_eps          = 1e-5         # Adam optimizer epsilon value\n",
    "opt_clip         = .5           # Amount to clip gradients by\n",
    "actor_coeff      = 1.           # Loss coefficient of the actor loss (for scaling the different loss components)\n",
    "critic_coeff     = .5           # Loss coefficient of the critic loss\n",
    "entropy_coeff    = .02          # Loss coefficient of the entropy term\n",
    "pp_coeff         = 1.           # Loss coefficient of the predictive processing loss\n",
    "hidden_size      = 512          # Number of latent hidden units before the LSTM layer\n",
    "rnn_hidden_size  = 1024         # Number of hidden units in the LSTM\n",
    "num_workers      = 16           # Number of parallel environments running\n",
    "batch_steps      = 125\n",
    "c, w, h          = 1, 84, 84\n",
    "gamma            = .99\n",
    "lamda            = .95\n",
    "clip_range       = 0.10\n",
    "schedule_steps   = 10000\n",
    "cooldown_period  = 200\n",
    "epochs           = 4\n",
    "n_mini_batch     = 5\n",
    "pred_steps       = 3\n",
    "\n",
    "# Initialize other globals\n",
    "env             = gym.make(game)\n",
    "cur_eps         = np.zeros((num_workers), dtype=np.int32)\n",
    "batch_size      = num_workers * batch_steps\n",
    "mini_work_size  = batch_steps // n_mini_batch\n",
    "mini_batch_size = batch_size // n_mini_batch\n",
    "states          = np.zeros((num_workers, 1, stacked_frames*c, w, h), dtype=np.float32)\n",
    "states_new      = np.zeros((num_workers, 210, 160, 3), dtype=np.float32)\n",
    "lives           = np.zeros(num_workers, dtype=np.int32) + env.unwrapped.ale.lives()\n",
    "total_episodes  = 0\n",
    "all_grads       = []\n",
    "output_dir      = './logs/'+output_name+datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz5HKONykFox",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Colab tensorboard extension, uncomment if running in colab\n",
    "\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Colab tensorboard, uncomment if running in Colab\n",
    "\n",
    "#%tensorboard --logdir './logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMCell(rnn.HybridRecurrentCell):\n",
    "    # Long-Short Term Memory (LSTM) network cell modified for predictive processing\n",
    "\n",
    "    def __init__(self, hidden_size,\n",
    "                 i2h_weight_initializer=None, h2h_weight_initializer=None,\n",
    "                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',\n",
    "                 input_size=0, prefix=None, params=None, activation='tanh',\n",
    "                 recurrent_activation='sigmoid'):\n",
    "        super(LSTMCell, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        self._input_size = input_size\n",
    "        self.i2h_weight = self.params.get('i2h_weight', shape=(4*hidden_size, env.action_space.n),\n",
    "                                          init=i2h_weight_initializer,\n",
    "                                          allow_deferred_init=True)\n",
    "        self.h2h_weight = self.params.get('h2h_weight', shape=(4*hidden_size, hidden_size),\n",
    "                                          init=h2h_weight_initializer,\n",
    "                                          allow_deferred_init=True)\n",
    "        self.i2h_bias = self.params.get('i2h_bias', shape=(4*hidden_size,),\n",
    "                                        init=i2h_bias_initializer,\n",
    "                                        allow_deferred_init=True)\n",
    "        self.h2h_bias = self.params.get('h2h_bias', shape=(4*hidden_size,),\n",
    "                                        init=h2h_bias_initializer,\n",
    "                                        allow_deferred_init=True)\n",
    "        self.test = 1\n",
    "        self._activation = activation\n",
    "        self._recurrent_activation = recurrent_activation\n",
    "\n",
    "\n",
    "    def state_info(self, batch_size=0):\n",
    "        return [{'shape': (batch_size, self._hidden_size), '__layout__': 'NC'},\n",
    "                {'shape': (batch_size, self._hidden_size), '__layout__': 'NC'}]\n",
    "\n",
    "    def _alias(self):\n",
    "        return 'lstm'\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = '{name}({mapping})'\n",
    "        shape = self.i2h_weight.shape\n",
    "        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0])\n",
    "        return s.format(name=self.__class__.__name__,\n",
    "                        mapping=mapping,\n",
    "                        **self.__dict__)\n",
    "\n",
    "    def hybrid_forward(self, F, inputs, states, i2h_weight,\n",
    "                       h2h_weight, i2h_bias, h2h_bias):\n",
    "        # pylint: disable=too-many-locals\n",
    "        prefix = 't%d_'%self._counter\n",
    "        i2h = F.FullyConnected(data=inputs[:,hidden_size:], weight=i2h_weight, bias=i2h_bias,\n",
    "                               num_hidden=self._hidden_size*4, name=prefix+'i2h')\n",
    "        PE = F.elemwise_sub(states[0][:,:hidden_size], inputs[:,:hidden_size], name=prefix+'min0')\n",
    "        states[0] = F.concat(PE, states[0][:,hidden_size:], dim = 1)\n",
    "        h2h = F.FullyConnected(data=states[0], weight=h2h_weight, bias=h2h_bias,\n",
    "                               num_hidden=self._hidden_size*4, name=prefix+'h2h')\n",
    "\n",
    "\n",
    "        gates = F.elemwise_add(i2h, h2h, name=prefix+'plus0')\n",
    "        slice_gates = F.SliceChannel(gates, num_outputs=4, name=prefix+'slice')\n",
    "\n",
    "\n",
    "        in_gate = self._get_activation(\n",
    "            F, slice_gates[0], self._recurrent_activation, name=prefix+'i')\n",
    "        forget_gate = self._get_activation(\n",
    "            F, slice_gates[1], self._recurrent_activation, name=prefix+'f')\n",
    "        in_transform = self._get_activation(\n",
    "            F, slice_gates[2], self._activation, name=prefix+'c')\n",
    "        out_gate = self._get_activation(\n",
    "            F, slice_gates[3], self._recurrent_activation, name=prefix+'o')\n",
    "        next_c = F.elemwise_add(F.elemwise_mul(forget_gate, states[1], name=prefix+'mul0'),\n",
    "                                F.elemwise_mul(in_gate, in_transform, name=prefix+'mul1'),\n",
    "                                name=prefix+'state')\n",
    "        next_h = F.elemwise_mul(out_gate, F.Activation(next_c, act_type=self._activation, name=prefix+'activation0'),\n",
    "                                name=prefix+'out')\n",
    "\n",
    "        return next_h, [next_h, next_c]\n",
    "\n",
    "    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,\n",
    "               valid_length=None):\n",
    "        \"\"\"Unrolls an RNN cell across time steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        length : int\n",
    "            Number of steps to unroll.\n",
    "        inputs : Symbol, list of Symbol, or None\n",
    "            If `inputs` is a single Symbol (usually the output\n",
    "            of Embedding symbol), it should have shape\n",
    "            (batch_size, length, ...) if `layout` is 'NTC',\n",
    "            or (length, batch_size, ...) if `layout` is 'TNC'.\n",
    "\n",
    "            If `inputs` is a list of symbols (usually output of\n",
    "            previous unroll), they should all have shape\n",
    "            (batch_size, ...).\n",
    "        begin_state : nested list of Symbol, optional\n",
    "            Input states created by `begin_state()`\n",
    "            or output state of another cell.\n",
    "            Created from `begin_state()` if `None`.\n",
    "        layout : str, optional\n",
    "            `layout` of input symbol. Only used if inputs\n",
    "            is a single Symbol.\n",
    "        merge_outputs : bool, optional\n",
    "            If `False`, returns outputs as a list of Symbols.\n",
    "            If `True`, concatenates output across time steps\n",
    "            and returns a single symbol with shape\n",
    "            (batch_size, length, ...) if layout is 'NTC',\n",
    "            or (length, batch_size, ...) if layout is 'TNC'.\n",
    "            If `None`, output whatever is faster.\n",
    "        valid_length : Symbol, NDArray or None\n",
    "            `valid_length` specifies the length of the sequences in the batch without padding.\n",
    "            This option is especially useful for building sequence-to-sequence models where\n",
    "            the input and output sequences would potentially be padded.\n",
    "            If `valid_length` is None, all sequences are assumed to have the same length.\n",
    "            If `valid_length` is a Symbol or NDArray, it should have shape (batch_size,).\n",
    "            The ith element will be the length of the ith sequence in the batch.\n",
    "            The last valid state will be return and the padded outputs will be masked with 0.\n",
    "            Note that `valid_length` must be smaller or equal to `length`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : list of Symbol or Symbol\n",
    "            Symbol (if `merge_outputs` is True) or list of Symbols\n",
    "            (if `merge_outputs` is False) corresponding to the output from\n",
    "            the RNN from this unrolling.\n",
    "\n",
    "        states : list of Symbol\n",
    "            The new state of this RNN after this unrolling.\n",
    "            The type of this symbol is same as the output of `begin_state()`.\n",
    "        \"\"\"\n",
    "        # pylint: disable=too-many-locals\n",
    "        self.reset()\n",
    "\n",
    "        inputs, axis, F, batch_size = _format_sequence(length, inputs, layout, False)\n",
    "        begin_state = _get_begin_state(self, F, begin_state, inputs, batch_size)\n",
    "\n",
    "        states = begin_state\n",
    "        outputs = []\n",
    "        all_states = []\n",
    "        all_states_h = []\n",
    "        all_states_c = []\n",
    "\n",
    "        all_states_h.append(states[0])\n",
    "        all_states_c.append(states[1])\n",
    "\n",
    "        for i in range(length):\n",
    "            output, states = self(inputs[i], states)\n",
    "            outputs.append(output)\n",
    "            all_states_h.append(states[0])\n",
    "            all_states_c.append(states[1])\n",
    "            if valid_length is not None:\n",
    "                all_states.append(states)\n",
    "        if valid_length is not None:\n",
    "            states = [F.SequenceLast(F.stack(*ele_list, axis=0),\n",
    "                                     sequence_length=valid_length,\n",
    "                                     use_sequence_length=True,\n",
    "                                     axis=0)\n",
    "                      for ele_list in zip(*all_states)]\n",
    "            outputs = _mask_sequence_variable_length(F, outputs, length, valid_length, axis, True)\n",
    "        outputs, _, _, _ = _format_sequence(length, outputs, layout, merge_outputs)\n",
    "\n",
    "        return outputs, states, [all_states_h[:-1], all_states_c[:-1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gugumLP7kFoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define ResNet-based Encoder model\n",
    "\n",
    "class Encoder(gluon.Block):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.layers = {}\n",
    "            self.layers.items()\n",
    "            self.channel_list = [24,32,64,128]\n",
    "            for i, channels in enumerate(self.channel_list):\n",
    "                layer = str(i)\n",
    "                self.layers['conv'+layer] = nn.Conv2D(channels, 3, strides=1, padding=1)\n",
    "                self.layers['max'+layer] = nn.MaxPool2D(pool_size=3, strides=2,padding=1)\n",
    "                self.layers['res'+layer+'_0'] =  ResidualBlock(channels)\n",
    "                self.layers['res'+layer+'_1'] =  ResidualBlock(channels)\n",
    "\n",
    "            for key, val in self.layers.items():\n",
    "                self.register_child(self.layers[key])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, channels in enumerate(self.channel_list):\n",
    "            layer = str(i)\n",
    "            x = nd.relu(x)\n",
    "            x = self.layers['conv'+layer](x)\n",
    "            x = self.layers['max'+layer](x)\n",
    "            x = self.layers['res'+layer+'_0'](x)\n",
    "            x = self.layers['res'+layer+'_1'](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(gluon.Block):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.conv0 = nn.Conv2D(in_channels, 3, strides=1, padding=1)\n",
    "            self.conv1 = nn.Conv2D(in_channels, 3, strides=1, padding=1)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = nd.relu(x)\n",
    "        h = self.conv0(h)\n",
    "        h = nd.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        x = h+x\n",
    "        return x\n",
    "\n",
    "\n",
    "# Main model definition\n",
    "\n",
    "class Model(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.enc = Encoder()\n",
    "            self.dense = nn.Dense(hidden_size)\n",
    "            self.lstm = LSTMCell(rnn_hidden_size)\n",
    "            self.flat = nn.Flatten()\n",
    "            self.action = nn.Dense(env.action_space.n)\n",
    "            self.value = nn.Dense(1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size, timesteps, stacked_frames, H, W = x.shape\n",
    "        x = x.reshape(batch_size*timesteps, stacked_frames, H, W)\n",
    "        x = nd.relu(self.enc(x))\n",
    "        rnn_input = nd.tanh(self.dense(x))\n",
    "        input_and_action = nd.concat(rnn_input, nd.zeros((batch_size*timesteps,env.action_space.n), dtype=np.float32).as_in_context(context), dim = 1)\n",
    "        input_and_action = input_and_action.reshape(batch_size, timesteps, -1)\n",
    "        x, hidden, all_hidden = self.lstm.unroll(length=timesteps, inputs=input_and_action, layout='NTC', begin_state=hidden, merge_outputs=True)\n",
    "\n",
    "        rnn_output = x.reshape(batch_size*timesteps, -1)\n",
    "        probs = self.action(rnn_output)\n",
    "        values = self.value(rnn_output)\n",
    "\n",
    "        return nd.softmax(probs.astype(np.float64)).astype(np.float32), values, hidden, rnn_input, rnn_output, all_hidden\n",
    "\n",
    "    def rnn_only(self, rnn_input, hidden):\n",
    "        input_and_action = nd.concat(rnn_input, nd.zeros((num_workers,env.action_space.n), dtype=np.float32).as_in_context(context), dim = 1)\n",
    "        input_and_action = input_and_action.reshape(len(rnn_input),1, -1)\n",
    "        x, hidden, _ = self.lstm.unroll(length=1, inputs=input_and_action, layout='NTC', begin_state=hidden, merge_outputs=True)\n",
    "        rnn_output = x.reshape(len(rnn_input), -1)\n",
    "        probs = self.action(rnn_output)\n",
    "        values = self.value(rnn_output)\n",
    "        return nd.softmax(probs.astype(np.float64)).astype(np.float32), values, hidden, rnn_output\n",
    "    \n",
    "    def rnn_input_pred(self, rnn_input, hidden, action, steps):\n",
    "        \n",
    "        # Reshape to workers format and only take the relevant inputs for each\n",
    "        rnn_input = rnn_input.reshape(num_workers, mini_work_size, hidden_size)[:,:-pred_steps]\n",
    "        # Concatenate all worker data\n",
    "        rnn_input = rnn_input.reshape(rnn_input.shape[0] * rnn_input.shape[1], hidden_size)     \n",
    "        \n",
    "        # Concatenate hiddens\n",
    "        hidden = [nd.reshape(hidden[0],(hidden[0].shape[0]*hidden[0].shape[1], rnn_hidden_size)), nd.reshape(hidden[1],(hidden[1].shape[0]*hidden[1].shape[1], rnn_hidden_size))]\n",
    "        preds = []\n",
    "        rnn_input_pred = rnn_input # Set to original input for first step\n",
    "        for step in range(steps):\n",
    "            cur_act = action[:,step:-(pred_steps-step)] # Get the relevant action for this step\n",
    "            cur_act = cur_act.reshape(cur_act.shape[0] * cur_act.shape[1]) # Reshape to concatenate\n",
    "            cur_act = nd.one_hot(cur_act, env.action_space.n, dtype=np.float32)\n",
    "            input_and_action = nd.concat(rnn_input_pred, cur_act.as_in_context(context).reshape(len(rnn_input_pred),-1), dim = 1)\n",
    "            input_and_action = input_and_action.reshape(len(rnn_input_pred), 1, -1)\n",
    "            x, hidden, _ = self.lstm.unroll(length=1, inputs=input_and_action, layout='NTC', begin_state=hidden, merge_outputs=True)\n",
    "            rnn_output = x.reshape(len(rnn_input_pred), -1)\n",
    "            rnn_input_pred = rnn_output[:,:hidden_size]\n",
    "            preds.append(rnn_input_pred)\n",
    "        return preds\n",
    "\n",
    "    def encode(self, x):\n",
    "        batch_size, timesteps, stacked_frames, H, W = x.shape\n",
    "        x = x.reshape(batch_size*timesteps, stacked_frames, H, W)\n",
    "        encoded = nd.relu(self.enc(x))\n",
    "        encoded = nd.tanh(self.dense(encoded))\n",
    "        return encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qs79QGcSkFo2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Multiprocessing setup\n",
    "\n",
    "class Game(object):\n",
    "    def __init__(self, game):\n",
    "        self.env = gym.make(game)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def step(self, action):\n",
    "        #self.env.render()\n",
    "        return self.env.step(action)\n",
    "\n",
    "\n",
    "def runner_process(remote, game):\n",
    "    game = Game(game)\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == \"step\":\n",
    "            remote.send(game.step(data))\n",
    "        elif cmd == \"reset\":\n",
    "            remote.send(game.reset())\n",
    "        elif cmd == \"close\":\n",
    "            remote.close()\n",
    "            break\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, game):\n",
    "        self.child, parent = multiprocessing.Pipe()\n",
    "        self.process = multiprocessing.Process(target=runner_process, args=(parent, game))\n",
    "        self.process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TVbkTQv1kFo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MxBoard / Tensorboard Monitoring Setup\n",
    "\n",
    "class Monitoring:\n",
    "    def __init__(self, output_dir):\n",
    "        self.update = []\n",
    "        self.total_episodes = []\n",
    "        self.rewards = []\n",
    "        self.mean100 = []\n",
    "        self.max_reward = []\n",
    "        self.update_max_reward = []\n",
    "        self.update_rewards = []\n",
    "        self.update_mean100 = []\n",
    "        self.critic_loss = []\n",
    "        self.actor_loss = []\n",
    "        self.pp_loss = []\n",
    "        self.entropy_loss = []\n",
    "        self.min_action_prob = []\n",
    "        self.max_action_prob = []\n",
    "        self.avg_action_prob = []\n",
    "        self.std_action_prob = []\n",
    "        self.avg_value = []\n",
    "        self.min_value = []\n",
    "        self.max_value = []\n",
    "        self.std_value = []\n",
    "        self.ratio = []\n",
    "        self.entropy_loss_buffer = []\n",
    "        self.pp_loss_buffer = []\n",
    "        self.actor_loss_buffer = []\n",
    "        self.critic_loss_buffer = []\n",
    "        self.ratio_buffer = []\n",
    "        self.sw = SummaryWriter(logdir=output_dir, flush_secs=5)\n",
    "\n",
    "\n",
    "    def process_episode(self, rewards, ep):\n",
    "        self.rewards.append(rewards)\n",
    "        self.total_episodes.append(ep)\n",
    "        if len(self.rewards)>100:\n",
    "            self.mean100.append(np.mean(self.rewards[-100:]))\n",
    "        else:\n",
    "            self.mean100.append(np.mean(self.rewards))\n",
    "        self.max_reward.append(np.max(self.rewards))\n",
    "\n",
    "    def process_rollout(self, data):\n",
    "        for key, val in data.items():\n",
    "            val = val.reshape(val.shape[0] * val.shape[1], *val.shape[2:])\n",
    "        self.min_action_prob.append(np.min(data['action_dists']))\n",
    "        self.max_action_prob.append(np.max(data['action_dists']))\n",
    "        self.avg_action_prob.append(np.mean(np.exp(data['log_probs'])))\n",
    "        self.std_action_prob.append(np.std(data['action_dists']))\n",
    "        self.avg_value.append(np.mean(data['values']))\n",
    "        self.min_value.append(np.min(data['values']))\n",
    "        self.max_value.append(np.max(data['values']))\n",
    "        self.std_value.append(np.std(data['values']))\n",
    "\n",
    "    def process_minibatch_loss(self, entropy_loss, pp_loss, actor_loss, critic_loss, ratio):\n",
    "        self.entropy_loss_buffer.append(entropy_loss)\n",
    "        self.actor_loss_buffer.append(actor_loss)\n",
    "        self.critic_loss_buffer.append(critic_loss)\n",
    "        self.ratio_buffer.append(ratio)\n",
    "        self.pp_loss_buffer.append(pp_loss)\n",
    "\n",
    "    def process_update(self, update):\n",
    "        self.update.append(update)\n",
    "        self.entropy_loss.append(np.mean(self.entropy_loss_buffer))\n",
    "        self.critic_loss.append(np.mean(self.critic_loss_buffer))\n",
    "        self.actor_loss.append(np.mean(self.actor_loss_buffer))\n",
    "        self.pp_loss.append(np.mean(self.pp_loss_buffer))\n",
    "        self.ratio.append(np.mean(self.ratio_buffer))\n",
    "        self.entropy_loss_buffer = []\n",
    "        self.actor_loss_buffer = []\n",
    "        self.critic_loss_buffer = []\n",
    "        self.ratio_buffer = []\n",
    "        self.pp_loss_buffer = []\n",
    "\n",
    "    def update_mxboard(self):\n",
    "        self.sw.add_scalar(tag='Losses/Critic_Loss',                           value=self.critic_loss[-1],         global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Losses/Actor_Loss',                            value=self.actor_loss[-1],          global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Losses/Entropy_Loss',                          value=self.entropy_loss[-1],        global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Losses/PP_loss',                               value=self.pp_loss[-1],             global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Probabilities/Average_Action_Probability',     value=self.avg_action_prob[-1],     global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Probabilities/Min._Action_Probability',        value=self.min_action_prob[-1],     global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Probabilities/Max._Action_Probability',        value=self.max_action_prob[-1],     global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Probabilities/Std._Dev._Action_Probability',   value=self.std_action_prob[-1],     global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Values/Average_State_Value',                   value=self.avg_value[-1],           global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Values/Min._State_Value',                      value=self.min_value[-1],           global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Values/Max._State_Value',                      value=self.max_value[-1],           global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Values/Std._Dev._State_Value',                 value=self.std_value[-1],           global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Ratio/Maximum_Ratio',                          value=self.ratio[-1],               global_step=self.update[-1])\n",
    "        if self.mean100:\n",
    "            self.update_max_reward.append(self.max_reward[-1])\n",
    "            self.update_rewards.append(self.rewards[-1])\n",
    "            self.update_mean100.append(self.mean100[-1])\n",
    "            self.sw.add_scalar(tag='Rewards/Rewards',                              value=self.rewards[-1],             global_step=self.update[-1])\n",
    "            self.sw.add_scalar(tag='Rewards/Avg._Reward_Last_100',                 value=self.mean100[-1],             global_step=self.update[-1])\n",
    "            self.sw.add_scalar(tag='Rewards/Max._Reward',                          value=self.max_reward[-1],          global_step=self.update[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "u9mnjLr_kFo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "\n",
    "def preprocess(state):\n",
    "    state = cv2.resize(cv2.cvtColor(state, cv2.COLOR_RGB2GRAY), (h, w), interpolation=cv2.INTER_AREA)\n",
    "    state = state / 255\n",
    "    return state\n",
    "\n",
    "def summarize(net, context):\n",
    "    state = env.reset()\n",
    "    state = preprocess(state)\n",
    "    state = np.array([state,]*stacked_frames).reshape((1, 1,stacked_frames, h, w))\n",
    "    net.summary(nd.array(state, ctx=context))\n",
    "\n",
    "def reset_state(runner):\n",
    "    runner.child.send((\"reset\", None))\n",
    "    state = runner.child.recv()\n",
    "    state = preprocess(state)\n",
    "    state = np.array([state,]*stacked_frames)\n",
    "    return state\n",
    "\n",
    "def process_states(states_new, states, dones, infos, runners, game, lives):\n",
    "    global cur_eps\n",
    "    global total_episodes\n",
    "    global monitor\n",
    "    states_new = np.stack(states_new)\n",
    "    states_new = np.array([preprocess(state) for state in states_new])\n",
    "    states = np.append(states, states_new.reshape(num_workers,1,1,h,w), axis=2)\n",
    "    states = np.delete(states, 0, axis=2)\n",
    "    for idx, [cur_done, runner, cur_info] in enumerate(zip(dones, runners, infos)):\n",
    "        if cur_done or (cur_info['ale.lives'] < lives[idx]):\n",
    "            lives[idx] = cur_info['ale.lives']\n",
    "            if cur_done:\n",
    "                monitor.process_episode(cur_eps[idx], total_episodes)\n",
    "                total_episodes += 1\n",
    "                cur_eps[idx] = 0\n",
    "                lives[idx] = env.unwrapped.ale.lives()\n",
    "                states[idx] = reset_state(runner)\n",
    "            dones[idx] = True\n",
    "    return states, dones\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def standardize(adv):\n",
    "    return (adv - adv.mean()) / (nd.sqrt(nd.power(adv-adv.mean(),2).sum() / len(adv)) + 1e-10)\n",
    "\n",
    "def calculate_advantages(done, rewards, values, states, hidden):\n",
    "    advantages = np.zeros((values.shape[0], values.shape[1]), dtype=np.float32)\n",
    "    next_advantage = 0\n",
    "\n",
    "    # Get the value of the state that resulted from the last action\n",
    "    _, next_value, *_ = net_new(nd.array(states, ctx=context, dtype=np.float32).reshape(num_workers, 1, stacked_frames*c, h, w),hidden)\n",
    "    next_value = next_value.reshape(-1).asnumpy()\n",
    "\n",
    "    # Work backwards through values to calculate GAE\n",
    "    # Whenever a life was lost or an episode ended it will have been marked with done\n",
    "    # Using this as a mask allows us to restart advantage calculation from these points\n",
    "    for t in reversed(range(values.shape[1])):\n",
    "        mask = 1.0 - done[:, t]\n",
    "        delta = rewards[:, t] + gamma * next_value * mask - values[:, t]\n",
    "        advantages[:, t] = delta + gamma * lamda * next_advantage * mask\n",
    "\n",
    "        next_advantage = advantages[:, t]\n",
    "        next_value = values[:, t]\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5-J3XrrBkFo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "def calc_actor_loss(probs, mini_batch, clip_range, advantages):\n",
    "\n",
    "    # Get log probabilities and PPO style clip the ratio between original model and current output\n",
    "    log_probs = nd.log(nd.pick(probs,mini_batch['actions'])+1e-10)\n",
    "    ratio = nd.exp(log_probs - mini_batch['log_probs'].detach())\n",
    "    clipped_ratio = nd.clip(ratio,1.0 - clip_range,1.0 + clip_range)\n",
    "\n",
    "    # Take the minimum result of the base ratio and the clipped ratio\n",
    "    actor_loss = nd.concat((ratio * advantages.detach()).reshape(1,-1),(clipped_ratio * advantages.detach()).reshape(1,-1), dim = 0)\n",
    "    actor_loss = nd.min(actor_loss, axis=0)\n",
    "    actor_loss = -actor_loss.mean()\n",
    "    return actor_loss, ratio\n",
    "\n",
    "def calc_critic_loss(mini_batch, value, clip_range):\n",
    "    # Calculate the batch return and clip the current value to stabilize the critic  \n",
    "    batch_return = mini_batch['values'] + mini_batch['advantages']\n",
    "    clipped_value = mini_batch['values'].detach() + nd.clip(value.reshape(-1) - mini_batch['values'].detach(), -clip_range, clip_range)\n",
    "    critic_loss = nd.abs(clipped_value - batch_return.detach())\n",
    "    critic_loss = critic_loss.mean()\n",
    "    return critic_loss\n",
    "\n",
    "def calc_entropy_loss(probs):\n",
    "    probs = probs+1e-10\n",
    "    entropy_loss = -(probs * probs.log()).sum(axis=1)\n",
    "    entropy_loss = entropy_loss.mean()\n",
    "    return entropy_loss\n",
    "\n",
    "\n",
    "def calc_pp_loss(predictions, rnn_input):\n",
    "    total_loss = nd.array([0.], ctx=context)\n",
    "    for step in range(pred_steps):\n",
    "        if step+1 == pred_steps:\n",
    "            target_rnn_input = rnn_input[:,step+1:]\n",
    "        else:\n",
    "            target_rnn_input = rnn_input[:,step+1:-(pred_steps-(step+1))]\n",
    "        pp_loss = nd.mean((predictions[step] - target_rnn_input.reshape(target_rnn_input.shape[0]*target_rnn_input.shape[1],-1))**2)\n",
    "        total_loss = total_loss + pp_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KtZOp8fEkFo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Batch rollout\n",
    "\n",
    "def rollout():\n",
    "    global states\n",
    "    global cur_eps\n",
    "    global lives\n",
    "    global hidden\n",
    "    global hiddens\n",
    "\n",
    "    # Initialize batch\n",
    "    data = {'rewards':         np.zeros((num_workers, batch_steps)),\\\n",
    "            'values':          np.zeros((num_workers, batch_steps)),\\\n",
    "            'log_probs':       np.zeros((num_workers, batch_steps)),\\\n",
    "            'action_dists':    np.zeros((num_workers, batch_steps, env.action_space.n)),\\\n",
    "            'done':            np.zeros((num_workers, batch_steps)),\\\n",
    "            'states':          np.zeros((num_workers, batch_steps, stacked_frames * c, h, w)),\\\n",
    "            'actions':         np.zeros((num_workers, batch_steps), dtype=np.int32),\\\n",
    "            'rnn_inputs':      np.zeros((num_workers, batch_steps, hidden_size))}\n",
    "    hiddens =  nd.zeros((num_workers, n_mini_batch, 2, rnn_hidden_size), ctx=context)\n",
    "\n",
    "    for step in range(batch_steps):\n",
    "\n",
    "        if step % mini_work_size == 0:\n",
    "            hiddens[:, int(step / mini_work_size), 0] = hidden[0].asnumpy()\n",
    "            hiddens[:, int(step / mini_work_size), 1] = hidden[1].asnumpy()\n",
    "\n",
    "        # Forward pass\n",
    "        probs, v, hidden, rnn_input, rnn_output, *_ = net(nd.array(states, ctx=context, dtype=np.float32),hidden)\n",
    "\n",
    "        # Sample action\n",
    "        act = mx.nd.sample_multinomial(probs)\n",
    "        act_probs = nd.pick(probs, act)\n",
    "        log_probs = nd.log(act_probs+1e-10)\n",
    "\n",
    "        # Store the new values in the batch data\n",
    "        data['rnn_inputs'][:, step]   = rnn_input.asnumpy().reshape(num_workers, -1) # step-1 because every rnn_input will be the t+1 target for input prediction\n",
    "        data['states'][:, step]       = states.reshape(num_workers, stacked_frames * c, h, w)\n",
    "        data['action_dists'][:, step] = probs.asnumpy()\n",
    "        data['values'][:, step]       = v.reshape(-1).asnumpy()\n",
    "        data['actions'][:, step]      = act.reshape(-1).asnumpy()\n",
    "        data['log_probs'][:, step]    = log_probs.reshape(-1).asnumpy()\n",
    "\n",
    "        # Execute the chosen actions in the workers and retrieve + process the next states\n",
    "        for idx, runner in enumerate(runners):\n",
    "            runner.child.send((\"step\", data['actions'][idx,step]))\n",
    "        states_new, data['rewards'][:, step], data['done'][:, step], info = np.transpose(np.array([runner.child.recv() for runner in runners], dtype='object'))\n",
    "        cur_eps = cur_eps + data['rewards'][:, step] # Track cumulative reward of the running episodes\n",
    "        states, data['done'][:, step] = process_states(states_new, states, data['done'][:, step], info, runners, game, lives)\n",
    "\n",
    "    # Process rollout for monitoring and convert to mx ndarray for backwards pass\n",
    "    monitor.process_rollout(data)\n",
    "    for key, val in data.items():\n",
    "        data[key] = nd.array(val, ctx=context, dtype=np.float32)\n",
    "\n",
    "    return data, hiddens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SiuT3MHakFo5",
    "outputId": "4b5fd923-4d4c-4b93-fdbf-230263e1aa28",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)                                Output Shape         Param #\n",
      "================================================================================\n",
      "               Input  (16, 1, 4, 84, 84), [(16, 1024), (16, 1024)]               0\n",
      "            Conv2D-1                            (16, 24, 84, 84)             888\n",
      "         MaxPool2D-2                            (16, 24, 42, 42)               0\n",
      "            Conv2D-3                            (16, 24, 42, 42)            5208\n",
      "            Conv2D-4                            (16, 24, 42, 42)            5208\n",
      "     ResidualBlock-5                            (16, 24, 42, 42)               0\n",
      "            Conv2D-6                            (16, 24, 42, 42)            5208\n",
      "            Conv2D-7                            (16, 24, 42, 42)            5208\n",
      "     ResidualBlock-8                            (16, 24, 42, 42)               0\n",
      "            Conv2D-9                            (16, 32, 42, 42)            6944\n",
      "        MaxPool2D-10                            (16, 32, 21, 21)               0\n",
      "           Conv2D-11                            (16, 32, 21, 21)            9248\n",
      "           Conv2D-12                            (16, 32, 21, 21)            9248\n",
      "    ResidualBlock-13                            (16, 32, 21, 21)               0\n",
      "           Conv2D-14                            (16, 32, 21, 21)            9248\n",
      "           Conv2D-15                            (16, 32, 21, 21)            9248\n",
      "    ResidualBlock-16                            (16, 32, 21, 21)               0\n",
      "           Conv2D-17                            (16, 64, 21, 21)           18496\n",
      "        MaxPool2D-18                            (16, 64, 11, 11)               0\n",
      "           Conv2D-19                            (16, 64, 11, 11)           36928\n",
      "           Conv2D-20                            (16, 64, 11, 11)           36928\n",
      "    ResidualBlock-21                            (16, 64, 11, 11)               0\n",
      "           Conv2D-22                            (16, 64, 11, 11)           36928\n",
      "           Conv2D-23                            (16, 64, 11, 11)           36928\n",
      "    ResidualBlock-24                            (16, 64, 11, 11)               0\n",
      "           Conv2D-25                           (16, 128, 11, 11)           73856\n",
      "        MaxPool2D-26                             (16, 128, 6, 6)               0\n",
      "           Conv2D-27                             (16, 128, 6, 6)          147584\n",
      "           Conv2D-28                             (16, 128, 6, 6)          147584\n",
      "    ResidualBlock-29                             (16, 128, 6, 6)               0\n",
      "           Conv2D-30                             (16, 128, 6, 6)          147584\n",
      "           Conv2D-31                             (16, 128, 6, 6)          147584\n",
      "    ResidualBlock-32                             (16, 128, 6, 6)               0\n",
      "          Encoder-33                             (16, 128, 6, 6)               0\n",
      "            Dense-34                                   (16, 512)         2359808\n",
      "         LSTMCell-35        (16, 1024), [(16, 1024), (16, 1024)]         4276224\n",
      "            Dense-36                                    (16, 18)           18450\n",
      "            Dense-37                                     (16, 1)            1025\n",
      "            Model-38  (16, 18), (16, 1), [(16, 1024), (16, 1024)], (16, 512), (16, 1024), [[(16, 1024)], [(16, 1024)]]               0\n",
      "================================================================================\n",
      "Parameters in forward computation graph, duplicate included\n",
      "   Total params: 7551563\n",
      "   Trainable params: 7551563\n",
      "   Non-trainable params: 0\n",
      "Shared params in forward computation graph: 0\n",
      "Unique parameters in model: 7551563\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:08:37] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and prepare for main loop\n",
    "\n",
    "net = Model()\n",
    "net.initialize(ctx=context)\n",
    "optimizer = gluon.Trainer(net.collect_params(), 'Adam', {'learning_rate': opt_lr, 'epsilon' : opt_eps})\n",
    "net_new = Model()\n",
    "net_new.initialize(ctx=context)\n",
    "\n",
    "optimizer_new = gluon.Trainer(net_new.collect_params(), 'Adam', {'learning_rate': opt_lr, 'epsilon' : opt_eps})\n",
    "monitor = Monitoring(output_dir)\n",
    "hidden = mx.nd.random.uniform(shape=(num_workers, rnn_hidden_size), ctx=context, dtype=np.float32)\n",
    "hidden = [hidden, hidden] # Gluon LSTM expects a list of recurrent state tensors (h0, c0)\n",
    "\n",
    "net.summary(nd.array(states, ctx=context, dtype=np.float32), hidden)\n",
    "probs, value, _, rnn_input, rnn_output, *_ = net_new(nd.array(states, ctx=context, dtype=np.float32), hidden)\n",
    "\n",
    "\n",
    "# Copy parameters between nets\n",
    "params1 = net_new.collect_params()\n",
    "params2 = net.collect_params()\n",
    "for p1, p2 in zip(params1.values(), params2.values()):\n",
    "    p2.set_data(p1.data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-lq8dOBLkFo6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize worker processes\n",
    "\n",
    "runners = [Runner(game) for i in range(num_workers)]\n",
    "for i, runner in enumerate(runners):\n",
    "    states[i] = reset_state(runner).reshape(1,1,stacked_frames * c,h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVprN465kFo6",
    "outputId": "60775e85-1877-4fee-d36d-31b555f37fb0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89115c6ed3704d9783a61001826a93e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training..:   0%|          | 0/10200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main loop\n",
    "\n",
    "with tqdm(range(schedule_steps+cooldown_period), desc='Training..') as updates:\n",
    "\n",
    "    for update in updates:\n",
    "\n",
    "        # Update learning rate\n",
    "        if update<schedule_steps:\n",
    "            progress = update/schedule_steps\n",
    "            opt_lr = 2.5e-4 * (1 - progress)\n",
    "\n",
    "        # Fetch new batch of data\n",
    "        batch, hiddens = rollout()\n",
    "        batch['new_values'] = batch['values']\n",
    "        batch['advantages'] = nd.zeros((num_workers,batch_steps), ctx=context, dtype=np.float32)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "\n",
    "            for mbatch in range(0, n_mini_batch):\n",
    "\n",
    "                # Copy parameters between nets\n",
    "                params1 = net_new.collect_params()\n",
    "                params2 = net.collect_params()\n",
    "                for p1, p2 in zip(params1.values(), params2.values()):\n",
    "                    p2.set_data(p1.data())\n",
    "\n",
    "                    \n",
    "                first = mbatch * mini_work_size\n",
    "                final = first + mini_work_size\n",
    "\n",
    "                # Create and fill new mini_batch\n",
    "                mini_batch = {}\n",
    "                \n",
    "                # Reshape states to avoid mxnet slice dimension limit\n",
    "                for key, val in batch.items():\n",
    "                    mini_batch[key] = val[:,first:final]\n",
    "\n",
    "                # Retrieve relevant hidden states (only the first, to minimize usage of stale hidden states)\n",
    "                mb_initial_hidden = [hiddens[:, mbatch, 0], hiddens[:, mbatch, 1]] # adjust to gluon LSTM expected data format\n",
    "\n",
    "\n",
    "                with autograd.record():\n",
    "\n",
    "                    # Get updated outputs with latest model\n",
    "                    probs, value, _, rnn_input, rnn_output, all_hidden = net_new(mini_batch['states'], mb_initial_hidden)\n",
    "                    batch['new_values'][:,mbatch*mini_work_size:mbatch*mini_work_size+mini_work_size] = value.reshape(num_workers,-1).detach()\n",
    "                    \n",
    "                    # Update predictions for predictive processing\n",
    "                    all_hidden[0] = nd.stack(*all_hidden[0]).swapaxes(0,1)\n",
    "                    all_hidden[1] = nd.stack(*all_hidden[1]).swapaxes(0,1)\n",
    "                    pred_hidden = [all_hidden[0][:,:-pred_steps],all_hidden[1][:,:-pred_steps]]\n",
    "                    predictions = net_new.rnn_input_pred(rnn_input, pred_hidden, mini_batch['actions'], pred_steps)\n",
    "                    pp_loss = calc_pp_loss(predictions, rnn_input.reshape(num_workers,mini_work_size,-1))\n",
    "\n",
    "                    batch['advantages'][:,first:] = nd.array(calculate_advantages(batch['done'][:,first:].asnumpy(), batch['rewards'][:,first:].asnumpy(), batch['new_values'][:,first:].asnumpy(), states, hidden), ctx=context)\n",
    "                    mini_batch['advantages'] = batch['advantages'][:,first:final]\n",
    "                    \n",
    "                    # Concatenate all worker data for further loss calculation\n",
    "                    for key, val in mini_batch.items():\n",
    "                        mini_batch[key] = val.reshape(val.shape[0] * val.shape[1], *val.shape[2:])\n",
    "\n",
    "                    standardized_adv = standardize(mini_batch['advantages'])\n",
    "\n",
    "                    # Calculate losses\n",
    "                    actor_loss, ratio  = calc_actor_loss(probs, mini_batch, clip_range, standardized_adv)\n",
    "                    critic_loss  = calc_critic_loss(mini_batch, value, clip_range)\n",
    "                    entropy_loss = calc_entropy_loss(probs)\n",
    "\n",
    "                    # Total loss\n",
    "                    loss = actor_coeff * actor_loss \\\n",
    "                         + critic_coeff * critic_loss  \\\n",
    "                         + pp_coeff * pp_loss \\\n",
    "                         - entropy_coeff * entropy_loss\n",
    "\n",
    "                    optimizer_new.set_learning_rate(opt_lr)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                grads = [i.grad(context) for i in net_new.collect_params().values() if i._grad is not None]\n",
    "                gluon.utils.clip_global_norm(grads, opt_clip)\n",
    "                optimizer_new.step(1)\n",
    "\n",
    "                # Update Tensorboard and logging\n",
    "                monitor.process_minibatch_loss(entropy_coeff * entropy_loss.asnumpy()[0],\\\n",
    "                                               pp_coeff * pp_loss.asnumpy()[0],\\\n",
    "                                               actor_coeff * actor_loss.asnumpy()[0],\\\n",
    "                                               critic_coeff * critic_loss.asnumpy()[0],\\\n",
    "                                               np.max(ratio.asnumpy()))\n",
    "\n",
    "        monitor.process_update(update)\n",
    "        monitor.update_mxboard()\n",
    "        if monitor.mean100: \n",
    "            updates.set_description('Training.. (Avg. last 100: %.2f)' % monitor.mean100[-1])\n",
    "        if update % 1000 == 0:\n",
    "            save_data = monitor.__dict__.copy()\n",
    "            del save_data[\"sw\"]\n",
    "            pickle.dump(save_data, open( output_dir + \"/monitor\" + str(update) + \".pkl\", \"wb\" ) )\n",
    "            net.save_parameters(output_dir + \"/net\"+ str(update) +\".params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytKXwyZikFo6",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_data = monitor.__dict__.copy()\n",
    "del save_data[\"sw\"]\n",
    "pickle.dump(save_data, open( output_dir + \"/monitor.pkl\", \"wb\" ) )\n",
    "net.save_parameters(output_dir + \"/net.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
