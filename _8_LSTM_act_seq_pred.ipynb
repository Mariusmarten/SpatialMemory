{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9963b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Func\n",
    "from torch.autograd import Variable\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491b8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/oracle_data.pickle\", \"rb\") as handle:\n",
    "    oracle_data = pickle.load(handle)\n",
    "\n",
    "with open(\"datasets/oracle_reversed_data.pickle\", \"rb\") as handle:\n",
    "    oracle_reversed_data = pickle.load(handle)\n",
    "\n",
    "with open(\"datasets/oracle_random_data.pickle\", \"rb\") as handle:\n",
    "    oracle_random_data = pickle.load(handle)\n",
    "\n",
    "with open(\"datasets/oracle_reversed_random_data.pickle\", \"rb\") as handle:\n",
    "    oracle_reversed_random_data = pickle.load(handle)\n",
    "\n",
    "with open(\"datasets/random_data.pickle\", \"rb\") as handle:\n",
    "    random_data = pickle.load(handle)\n",
    "\n",
    "with open(\"datasets/tmaze_random_reverse_data.pickle\", \"rb\") as handle:\n",
    "    tmaze_random_reverse_data = pickle.load(handle)\n",
    "\n",
    "with open(\"datasets/oracle_reversed_random_data_small.pickle\", \"rb\") as handle:\n",
    "    oracle_reversed_random_data_small = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aead960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = oracle_reversed_data['actions']\n",
    "actions = [[i] for i in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cea49ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(actions, \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e570c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        _x = data[i : (i + seq_length)]\n",
    "        _y = data[i + seq_length]  # _y = data[i+seq_length] (target)\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x), np.array(y)  # train, val. data\n",
    "\n",
    "training_data = actions\n",
    "\n",
    "\n",
    "seq_length = 20\n",
    "shuffle = False\n",
    "\n",
    "x, y = sliding_windows(training_data, seq_length)\n",
    "\n",
    "# one method of performing the training and validation split\n",
    "train_size = int(len(y) * 0.80)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "\n",
    "# shuffle data first before splitting\n",
    "if shuffle:\n",
    "    idx_shuf = list(range(len(y)))\n",
    "    random.shuffle(idx_shuf)\n",
    "    x = [x[i] for i in idx_shuf]\n",
    "    y = [y[i] for i in idx_shuf]\n",
    "\n",
    "# split data\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size : len(x)])))\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size : len(y)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c0d619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: [[1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "label: [2]\n"
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "print('sequence:', x[k])\n",
    "print('label:', y[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4140e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        h_out = h_out.view(-1, self.hidden_size)\n",
    "        out = self.fc(h_out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "num_epochs = 2000\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11e1b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length: 20\n",
      "input (trainX): tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.]]])\n",
      "tensor([[-0.0469,  0.0937,  0.0225,  0.1312]], grad_fn=<AddmmBackward0>)\n",
      "outputs: tensor([[-0.0469,  0.0937,  0.0225,  0.1312]], grad_fn=<AddmmBackward0>)\n",
      "output rounded: tensor([3])\n",
      "labels: tensor([[2.]])\n",
      "\n",
      "length input (trainX): 1895\n",
      "length outputs: 1895\n",
      "length labels: 1895\n",
      "\n",
      "SUMMARY LSTM \n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LSTM                                     [1908, 4]                 --\n",
      "├─LSTM: 1-1                              [1908, 1, 20]             1,840\n",
      "├─Linear: 1-2                            [1908, 4]                 84\n",
      "==========================================================================================\n",
      "Total params: 1,924\n",
      "Trainable params: 1,924\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 3.67\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.37\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.38\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "value = 0\n",
    "\n",
    "test_input = trainX[value:value+1]\n",
    "print(\"input length:\", len(test_input[0]))\n",
    "print(\"input (trainX):\", test_input)\n",
    "\n",
    "outputs = lstm(test_input)\n",
    "print(outputs)\n",
    "\n",
    "print(\"outputs:\", outputs)\n",
    "_, rounded_opt = torch.max(outputs, 1)\n",
    "print(\"output rounded:\", rounded_opt)\n",
    "print(\"labels:\", trainY[value:value+1])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "outputs = lstm(trainX)\n",
    "\n",
    "print(\"length input (trainX):\", len(trainX))\n",
    "print(\"length outputs:\", len(outputs))\n",
    "print(\"length labels:\", len(trainY))\n",
    "\n",
    "print(\"\")\n",
    "print(\"SUMMARY LSTM \\n\", summary(lstm, (1908, 1, 1), device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "692f1518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 1.39315, train acc: 0.31346, test loss: 1.35094, test acc: 0.83544\n",
      "Epoch: 100, train loss: 0.88021, train acc: 0.74354, test loss: 0.93697, test acc: 0.67089\n",
      "Epoch: 200, train loss: 0.52933, train acc: 0.82586, test loss: 0.50351, test acc: 0.83544\n",
      "Epoch: 300, train loss: 0.43009, train acc: 0.82639, test loss: 0.41443, test acc: 0.83544\n",
      "Epoch: 400, train loss: 0.36335, train acc: 0.83852, test loss: 0.35129, test acc: 0.86709\n",
      "Epoch: 500, train loss: 0.31922, train acc: 0.86069, test loss: 0.31269, test acc: 0.89662\n",
      "Epoch: 600, train loss: 0.30315, train acc: 0.87652, test loss: 0.28685, test acc: 0.89662\n",
      "Epoch: 700, train loss: 0.26798, train acc: 0.92612, test loss: 0.27023, test acc: 0.90928\n",
      "Epoch: 800, train loss: 0.25326, train acc: 0.93245, test loss: 0.25676, test acc: 0.92194\n",
      "Epoch: 900, train loss: 0.23796, train acc: 0.92612, test loss: 0.24527, test acc: 0.90717\n",
      "Epoch: 1000, train loss: 0.22688, train acc: 0.93087, test loss: 0.23523, test acc: 0.91983\n",
      "Epoch: 1100, train loss: 0.21320, train acc: 0.93720, test loss: 0.22509, test acc: 0.93460\n",
      "Epoch: 1200, train loss: 0.20116, train acc: 0.96148, test loss: 0.21462, test acc: 0.97257\n",
      "Epoch: 1300, train loss: 0.18924, train acc: 0.96939, test loss: 0.20201, test acc: 0.97257\n",
      "Epoch: 1400, train loss: 0.17676, train acc: 0.96939, test loss: 0.18585, test acc: 0.97257\n",
      "Epoch: 1500, train loss: 0.18161, train acc: 0.96148, test loss: 0.17736, test acc: 0.97257\n",
      "Epoch: 1600, train loss: 0.15912, train acc: 0.96939, test loss: 0.16223, test acc: 0.97257\n",
      "Epoch: 1700, train loss: 0.15251, train acc: 0.96939, test loss: 0.15472, test acc: 0.97257\n",
      "Epoch: 1800, train loss: 0.14763, train acc: 0.96939, test loss: 0.14915, test acc: 0.97257\n",
      "Epoch: 1900, train loss: 0.14372, train acc: 0.96939, test loss: 0.14477, test acc: 0.97257\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "best value: 0.07111 (hidden size of 20) - but overall little to no diff.\n",
    "best value: 0.06789 (with a seq. length of 20)\n",
    "increasing the length of the sequence length concidered \n",
    "for making the next decision, the better the resulting prediction\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#criterion = torch.nn.MSELoss()  # MSELoss - regression, CrossEntropyLoss for labels\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm(trainX)\n",
    "    labels = trainY.to(torch.long)\n",
    "    labels = labels[:, 0]\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    # compute train acc\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    train_acc = correct / len(trainX)\n",
    "    \n",
    "    # compute test acc\n",
    "    with torch.no_grad():\n",
    "        outputs = lstm(testX)\n",
    "        labels = testY.to(torch.long)\n",
    "        labels = labels[:, 0]\n",
    "        \n",
    "        loss_test = criterion(outputs, labels)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        test_acc = correct / len(testX)\n",
    "\n",
    "    optimizer.step()\n",
    "    if epoch % (num_epochs/20) == 0:\n",
    "        print(\"Epoch: %d, train loss: %1.5f, train acc: %1.5f, test loss: %1.5f, test acc: %1.5f\" % (epoch, loss.item(), train_acc, loss_test.item(), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2adbf07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "2369\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (474) must match the size of tensor b (2369) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m dataY\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m incorrect \u001b[38;5;241m=\u001b[39m [\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m]\n\u001b[1;32m      8\u001b[0m incorrect \u001b[38;5;241m=\u001b[39m incorrect[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (474) must match the size of tensor b (2369) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(predicted))\n",
    "print(len(dataY))\n",
    "\n",
    "labels = dataY.to(torch.long)\n",
    "labels = labels[:, 0]\n",
    "\n",
    "incorrect = [predicted == labels]\n",
    "incorrect = incorrect[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8468409",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.eval()\n",
    "predict = lstm(dataX) # all data\n",
    "\n",
    "_, predicted = torch.max(predict, 1)\n",
    "data_predict = predicted.data.numpy()\n",
    "\n",
    "dataY_plot = dataY.data.numpy()\n",
    "\n",
    "font = 24\n",
    "hfont = {'fontname':'Helvetica'}\n",
    "\n",
    "plt.rcParams['axes.titlepad'] = 13 \n",
    "plt.rcParams['xtick.major.pad']='8' # axis distance\n",
    "plt.rcParams['ytick.major.pad']='8'\n",
    "plt.rcParams.update({\"font.size\": 24})\n",
    "\n",
    "with plt.style.context(\"seaborn-darkgrid\"):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    plt.axvline(x=train_size, c=\"r\", linestyle=\"--\")\n",
    "\n",
    "    plt.plot(dataY_plot, \"o\", color='firebrick', markersize=10, label=\"actual\")\n",
    "    plt.plot(data_predict, \"o\", color=\"slategray\", markersize=5, label=\"predicted\")\n",
    "    \n",
    "    plt.yticks([0, 1, 2, 3], ['Left', 'Right', 'Forward', 'Backwards'])\n",
    "\n",
    "    # plt.legend(['Training Acc', 'Validation Acc'])\n",
    "    #plt.legend(loc=\"lower right\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title(\"Action sequence prediction\")\n",
    "    plt.xlabel(\"Sequence\", fontsize=font, **hfont)\n",
    "    plt.ylabel(\"Action\", fontsize=font, **hfont)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a2d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "pytorch_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
